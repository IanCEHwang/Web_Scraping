{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71db27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import pymongo\n",
    "from fastapi import FastAPI\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdaf6973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Best_buy_scrap:\n",
    "    def __init__(self , product_name):\n",
    "        self.search_term = product_name\n",
    "        \n",
    "    ### request search page information and return bs4 object\n",
    "    def get_search_pages(self , search_term , p_number):\n",
    "        ### URL\n",
    "        search_page_url = \"https://www.bestbuy.com/site/searchpage.jsp?id=pcat17071&st=\"\n",
    "        ### search product\n",
    "        search_product = search_term\n",
    "        \n",
    "        ### page_number\n",
    "        page_number = 0\n",
    "        if p_number == 0:\n",
    "            page_number = 1\n",
    "        else:\n",
    "            page_number = p_number\n",
    "\n",
    "        ### page_number search term\n",
    "        search_page_encoding = f\"&cp={page_number}\"\n",
    "\n",
    "        ### User agent\n",
    "        User_Agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36 Edg/88.0.705.56'\n",
    "\n",
    "        ### request access\n",
    "        r = requests.get(search_page_url + search_product + search_page_encoding , headers = {'User-Agent' : User_Agent})\n",
    "        \n",
    "        ### print url's\n",
    "        print(search_page_url + search_product + search_page_encoding)\n",
    "\n",
    "        ### Assign bs4 object to soup\n",
    "        soup = BeautifulSoup(r.content, 'lxml')\n",
    "        \n",
    "        #creating a file \"ebay_amazon gift cards_webpages\" for storing all the pages of seacrh results that we download. But before that we are checking if the we already have one file with the same name. If so they code piece will not run\n",
    "        try:\n",
    "            if not os.path.exists(\"best_buy_folder\"):\n",
    "                os.makedirs(\"best_buy_folder\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #downloading the first pages of seacrh results \n",
    "\n",
    "        response_page1 = requests.get(search_page_url + search_product + search_page_encoding , headers = {'User-Agent' : User_Agent})\n",
    "        soup_page1 = BeautifulSoup(response_page1.content, 'html.parser')\n",
    "        with open(f'best_buy_folder/best_buy_page_{page_number}.html', 'w', encoding='utf-8') as file:\n",
    "            file.write(str(soup_page1))\n",
    "\n",
    "        return soup\n",
    "    \n",
    "    ### handle price text\n",
    "    def handle_price(self , price):\n",
    "        try:\n",
    "            price = price.replace(\"$\" , \"\")\n",
    "            price = float(price)\n",
    "        except:\n",
    "            pass\n",
    "        return price\n",
    "\n",
    "    ### exrtract brand name from header\n",
    "    def get_brand(self , header):\n",
    "        try:\n",
    "            brand = header.split(\"-\")[0].strip(\" \")\n",
    "        except:\n",
    "            brand = \"\"\n",
    "        return brand\n",
    "    \n",
    "    ### get product info from bs4 object and return dict of info\n",
    "    def get_product_info(self , product):\n",
    "        return_dict = {}\n",
    "\n",
    "        ### get header\n",
    "        header = product.find('h4').text\n",
    "        return_dict.update({'header' : header})\n",
    "\n",
    "        ### get brand\n",
    "        brand = self.get_brand(header)\n",
    "        return_dict.update({'brand' : brand})\n",
    "\n",
    "        ### get labels\n",
    "        try:\n",
    "            labels = product.find('div' , attrs = {'class' : 'lv-stacked-carousel'}).find_all('button')\n",
    "            labels = [lab.text for lab in labels]\n",
    "        except:\n",
    "            labels = []\n",
    "        return_dict.update({'labels' : labels})\n",
    "\n",
    "\n",
    "        ### get model\n",
    "        model = product.find_all('span' , attrs = {'class' : 'sku-value'})[0].text\n",
    "        return_dict.update({'model' : model})\n",
    "\n",
    "        ### get sku\n",
    "        sku = product.find_all('span' , attrs = {'class' : 'sku-value'})[1].text\n",
    "        return_dict.update({'sku' : sku})\n",
    "        \n",
    "        ### get model link\n",
    "        link = product.find(\"div\" , attrs = {\"class\" : \"list-item lv\"}).find(\"a\", attrs = {\"class\" : \"image-link\"})[\"href\"]\n",
    "        return_dict.update({'product link' : \"https://www.bestbuy.com\"+link})\n",
    "                \n",
    "                            \n",
    "        ### get stars\n",
    "        try:\n",
    "            stars = product.find('p').text.split(' ')[1]\n",
    "            return_dict.update({'stars' : float(stars)})\n",
    "        except:\n",
    "            pass \n",
    "\n",
    "        ### get number of reviews\n",
    "        number_reviews = product.find('p').text.split(' ')[-2]\n",
    "        return_dict.update({'number_of_reivews' : number_reviews})\n",
    "\n",
    "        ### get price\n",
    "        try:\n",
    "            price = product.find(\"div\" , attrs = {\"class\" : \"priceView-hero-price priceView-customer-price\"}).find('span').text\n",
    "            return_dict.update({\"price\" : self.handle_price(price)})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        ### get old price\n",
    "        try:\n",
    "            old_price = product.find(\"div\" , attrs = {\"class\" : \"pricing-price__regular-price\"}).text.split(' ')[1]\n",
    "            return_dict.update({'original_price' : self.handle_price(old_price)})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        ### price diff\n",
    "        try:\n",
    "            reduction = self.handle_price(old_price) - self.handle_price(price)\n",
    "            return_dict.update({'price_reduction' : reduction})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return return_dict\n",
    "        \n",
    "        \n",
    "    def save_product(self, product , directory):       \n",
    "        product_dict = {}\n",
    "        ### get product webpage\n",
    "        try:\n",
    "            url = product['product link']\n",
    "            web_page = requests.get(url , headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36 Edg/88.0.705.56'})\n",
    "            product_lxml = BeautifulSoup(web_page.content , \"lxml\")\n",
    "        except:\n",
    "            return 0\n",
    "            \n",
    "        ### Title\n",
    "        try:\n",
    "            Title = product_lxml.find(\"div\" , attrs = {\"class\" : \"shop-product-title\"})\n",
    "            product_name = Title.find(\"div\" , attrs = {\"class\" : \"sku-title\"}).text\n",
    "            product_dict.update({\"product name\" : product_name})\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "        ### SKUID\n",
    "        try:\n",
    "            skuid = product_lxml.select_one(\"div.sku.product-data\").text.split(\":\")[1].strip(\" \")\n",
    "            product_dict.update({\"skuid\" : skuid})\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        ### Installment\n",
    "        try:\n",
    "            install_fh = product_lxml.find(\"div\" , attrs = {'class' : \"total-cost-clarity-content__monthly-payment\"}).text.split(\".\")[0]\n",
    "            install_sh = product_lxml.find(\"div\" , attrs = {'class' : \"total-cost-clarity-content__finance-message\"}).text\n",
    "            install_combine = install_fh + \" + \" + install_sh\n",
    "            product_dict.update({\"installment\" : install_combine})\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        ### Return period\n",
    "        try:\n",
    "            return_period = product_lxml.find(\"div\" , attrs = {'class' : \"product-return-message__label product-return-message__label--pdp product-return-message__label--pdp-large\"}).text\n",
    "            product_dict.update({\"return period\" : return_period})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        ### Warrant\n",
    "        try:\n",
    "            warrant = product_lxml.find(\"div\" , attrs = {'class' : \"warranty-list\"}).find_all(\"label\")\n",
    "            warrant = [deal.text for deal in warrant]\n",
    "            product_dict.update({\"warrant\" : warrant})\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        ### create directory\n",
    "        try:\n",
    "            os.mkdir(directory)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        ### save file\n",
    "        with open(f\"{directory}/{skuid}.htm\" , \"w\") as f:\n",
    "            f.write(str(web_page))\n",
    "            \n",
    "        return product_dict\n",
    "            \n",
    "    ### execute whole process\n",
    "    def execute_scrapping(self , directory):\n",
    "        re_list = []\n",
    "        advanced_info = []\n",
    "        stopper = False\n",
    "        counter = 1\n",
    "\n",
    "        while stopper != True:\n",
    "            page_n = self.get_search_pages(self.search_term , counter)\n",
    "            page_source = page_n.find_all('div' , attrs = {'class' : 'shop-sku-list-item'})\n",
    "            \n",
    "            if len(page_source) == 0:\n",
    "                stopper = True\n",
    "            else:\n",
    "                for product in page_source:\n",
    "                    ### get individual product dictionary\n",
    "                    product_info = self.get_product_info(product)\n",
    "                    re_list.append(product_info)\n",
    "                    \n",
    "                    ### save individual product advanced info + webpage\n",
    "                    advanced = self.save_product(product_info , directory)\n",
    "                    advanced_info.append(advanced)\n",
    "                    \n",
    "                    time.sleep(1)\n",
    "                        \n",
    "                print(f\"page {counter} done!\")\n",
    "                counter = counter + 1\n",
    "            \n",
    "            ### pause after request\n",
    "            time.sleep(1)\n",
    "        ### return dict of products\n",
    "        return re_list , advanced_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d29ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.bestbuy.com/site/searchpage.jsp?id=pcat17071&st=TV&cp=1\n",
      "page 1 done!\n",
      "https://www.bestbuy.com/site/searchpage.jsp?id=pcat17071&st=TV&cp=2\n",
      "page 2 done!\n",
      "https://www.bestbuy.com/site/searchpage.jsp?id=pcat17071&st=TV&cp=3\n",
      "page 3 done!\n",
      "https://www.bestbuy.com/site/searchpage.jsp?id=pcat17071&st=TV&cp=4\n",
      "page 4 done!\n",
      "https://www.bestbuy.com/site/searchpage.jsp?id=pcat17071&st=TV&cp=5\n",
      "page 5 done!\n",
      "https://www.bestbuy.com/site/searchpage.jsp?id=pcat17071&st=TV&cp=6\n",
      "page 6 done!\n",
      "https://www.bestbuy.com/site/searchpage.jsp?id=pcat17071&st=TV&cp=7\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    \n",
    "    ### initiate object\n",
    "    bb_init = Best_buy_scrap(\"TV\")\n",
    "    \n",
    "    ### start scrapping\n",
    "    dict_info , advanced_info = bb_init.execute_scrapping(\"Advanced_product_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f08064",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check results\n",
    "dict_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b173d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check advanced product info\n",
    "advanced_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62e7f87-3db2-4d34-a85d-22759aac436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB database\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# Get the \"bayc\" collection\n",
    "db = client[\"best_buy_db\"]\n",
    "\n",
    "# Drop the \"bayc\" collection if it exists\n",
    "if \"best_buy_collection\" in db.list_collection_names():\n",
    "    db[\"best_buy_collection\"].drop()\n",
    "\n",
    "# create a db \n",
    "collection = db[\"best_buy_collection\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e2083-dcdd-4fcd-b487-dff4d8f2437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pushing into collection\n",
    "collection.insert_many(dict_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27793a3-399d-4a0e-bb21-872a5bc841c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
